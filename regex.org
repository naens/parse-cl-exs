#+TITLE: Regular Expressions

* Overview

Regular expressions use a simple non-standard grammar.  It allows to
use basic unary and binary operators and sub-rules.  These regular
expressions are made to be used in grammars for scanners.  Each
expression corresponds to a rule or a subrule.  The difference between
both is that subrules are do not make an accepting state, so they can
be used as parts of rules.

* Operators

Here are the operators in order of precedence:
 * ~..~ : Range between characters.  The first or the last character
   can be missing.
 * ~*~, ~?~, ~+~: Kleene-closure, optional element, repetition
 * =~=: negation
 * ~&~, ~\~: conjunction and difference
 * ~\,\~: string concatenation
 * ~|~: union

* Rules and Subrules

Rules correspond to tokens, terminals and accepting states.  They are
defined ~<name>, '=', <regex>~.

Subrules are subexpressions that can be used by rules.  They are
defined like rules, but with a ~def~ prefix: ~\'def', <name>, '=',
<regex>~.

** Separation between rules
A rule definition begins with a rule identifier or a ~def~ keyword.
A rule identifier begins with a letter.  A letter not in quotes
and ~<~ ~>~ signs signifies the start of a new rule.  A rule / regex
can contain several lines and ends at the end of input or at the start
of a new rule.

* Definition of the grammar of the regular expressions

#+BEGIN_SRC
** Lexer grammar/* separators */
def space = <sp>+
def lnbrk = <nl>+
sep = <space> | <lnbrk>

/* operators */
op-repeat = <sep>*, '+', <sep>*
op-star = '*', <sep>*
op-option = '?', <sep>*
op-and = <sep>*, '&', <sep>*
op-sub = <sep>*, '\'
op-car = <sep>*, ','
op-range = '..'
op-neg = <sep>*, '~'
op-eq = <sep>*, '='

/* chracters */
def sp = 32h | 09h
def nl = 0ah, 0dh?
def alpha = 'a'..'z' | 'A'..'Z'
def digit = '0'..'9'
def hexdig = <digit> | 'a'..'f' | 'A'..'F'
def alhpanum = <alpha> | <digit>
def quote = ''''

/* tokens */
string = <quote>, (~<quote> & .. | <quote>, <quote>)+, <quote>
def hex1 = '0x', <hexdig>+
def hex2 = <digit>, <hexdigit>*, ('h' | 'H')
def dec-num = <digit>+
def number = <dec-num> | <hex1> | <hex2>
def char = <string> | <number>
range = <char>, (<op-range>, <char>?)?
      | <op-range>, <char>?
identifier = <alhpa>, ((<alphanum> | '-')*, <alphanum>)?
name = '<', <identifier>, '>'
comment = '/*', ~(..*, '*/', ..*), '*/'

#+END_SRC

** Grammar Syntax
#+BEGIN_SRC
/* non-terminals */
regex = <disjunct>, (<op-or>, <disjunct>)*
disjunct = <term>, (<op-cat>, <term>)*
term = <conjunct>, ((<op-and> | <op-diff>), <conjunct>)*
def specifier = <op-star> | <op-opt> | <op-rep>
conjunct = <op-neg>?, <element>, <specifier>?
def element = <range> | <string> | <ident> | <subexpr>
rule = <name>, <op-eq>, <regex>
#+END_SRC

* Included and excluded items in the token list
Only rules are included.  For a specific character to be included, they must
have their own rule.

* Grammars for parsers
Parser also use the same grammar as regular expressions with some
differences:
 * the rules correspond to the non-terminals, the subrules are still
   possible and also defined with defs
 * the rules that are not defined but used are terminals and are supposed to
   be defined by the tokenizer
 * some operations are not possible, the only possible operations are
   concatenation, disjunction, Kleene star, repetition and option.

These grammars can be read with the same grammar reader, but are interpreted
differently and using different tools.

* Modules
Different modules are responsible for the different stages of the reading of
the files and their transformation and interpretation:
 * Grammar Reader: reads grammars for tokenizers and parsers.  Outputs
   a list of rules.
 * Dirivator: reads the regular expression rules and outputs a DFA.
 * Tokenizer: reads a file using the DFA corresponding to the grammar that
   the input file is supposed to respect, outputs a list of tokens.
 * Parser: reads the list of tokens and outputs the AST.  It can also use
   the parser grammar, depending on the implementation.
